{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "383bfe31",
        "outputId": "6c70c67f-f154-4b97-8457-c4490a5d32d7"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/cleaned_comments_df.csv')\n",
        "display(df.head())\n",
        "display(df.info())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                 id                                       comment_text  toxic  \\\n",
              "0  0000997932d777bf  explanationwhy edit make username hardcore met...      0   \n",
              "1  000103f0d9cfb60f  ' aww ! match background colour ' seemingly st...      0   \n",
              "2  000113f07ec002fd  hey man , ' really try edit war . ' guy consta...      0   \n",
              "3  0001b41b1c6bb37e  \" morei ' make real suggestions improvement - ...      0   \n",
              "4  0001d958c54c6e35            , sir , hero . chance remember page ' ?      0   \n",
              "\n",
              "   severe_toxic  obscene  threat  insult  identity_hate  clean  char_length  \n",
              "0             0        0       0       0              0   True          264  \n",
              "1             0        0       0       0              0   True          112  \n",
              "2             0        0       0       0              0   True          233  \n",
              "3             0        0       0       0              0   True          622  \n",
              "4             0        0       0       0              0   True           67  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56a2bf71-8c85-419f-b5c7-52a669698c1d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>clean</th>\n",
              "      <th>char_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>explanationwhy edit make username hardcore met...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>' aww ! match background colour ' seemingly st...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>hey man , ' really try edit war . ' guy consta...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\" morei ' make real suggestions improvement - ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>, sir , hero . chance remember page ' ?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56a2bf71-8c85-419f-b5c7-52a669698c1d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-56a2bf71-8c85-419f-b5c7-52a669698c1d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-56a2bf71-8c85-419f-b5c7-52a669698c1d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-371d0608-c65e-4bcb-a611-e462a4265261\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-371d0608-c65e-4bcb-a611-e462a4265261')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-371d0608-c65e-4bcb-a611-e462a4265261 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"000103f0d9cfb60f\",\n          \"0001d958c54c6e35\",\n          \"000113f07ec002fd\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comment_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"' aww ! match background colour ' seemingly stick . thank . ( talk ) 21 : 51 , january 11 , 2016 ( utc )\",\n          \", sir , hero . chance remember page ' ?\",\n          \"hey man , ' really try edit war . ' guy constantly remove relevant information talk edit instead talk page . seem care format actual info .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"toxic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"severe_toxic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"obscene\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"threat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"insult\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"identity_hate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"char_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 218,\n        \"min\": 67,\n        \"max\": 622,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          112\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159571 entries, 0 to 159570\n",
            "Data columns (total 10 columns):\n",
            " #   Column         Non-Null Count   Dtype \n",
            "---  ------         --------------   ----- \n",
            " 0   id             159571 non-null  object\n",
            " 1   comment_text   159563 non-null  object\n",
            " 2   toxic          159571 non-null  int64 \n",
            " 3   severe_toxic   159571 non-null  int64 \n",
            " 4   obscene        159571 non-null  int64 \n",
            " 5   threat         159571 non-null  int64 \n",
            " 6   insult         159571 non-null  int64 \n",
            " 7   identity_hate  159571 non-null  int64 \n",
            " 8   clean          159571 non-null  bool  \n",
            " 9   char_length    159571 non-null  int64 \n",
            "dtypes: bool(1), int64(7), object(2)\n",
            "memory usage: 11.1+ MB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "None"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fece5444"
      },
      "source": [
        "**Reasoning**:\n",
        "Handle missing values, convert to list, and apply TF-IDF vectorization to the comment text data as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmvXl77Ss2HE"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "df['comment_text'] = df['comment_text'].fillna('')\n",
        "comment_list = df['comment_text'].tolist()\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=3,\n",
        "                                   max_features=None,\n",
        "                                   strip_accents='unicode',\n",
        "                                   analyzer='word',\n",
        "                                   token_pattern=r'\\w{1,}',\n",
        "                                   ngram_range=(1, 3),\n",
        "                                   use_idf=True,\n",
        "                                   smooth_idf=True,\n",
        "                                   sublinear_tf=True)\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(comment_list)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdd10327"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the features (tfidf_matrix) and the target variables into training and testing sets using train_test_split.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "88a40b2d",
        "outputId": "7ce4a4db-634f-433f-db52-7014c9bcb46b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target_variables = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "y = df[target_variables]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)\n",
        "\n",
        "display(X_train.shape)\n",
        "display(X_test.shape)\n",
        "display(y_train.shape)\n",
        "display(y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(127656, 396665)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(31915, 396665)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(127656, 6)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(31915, 6)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a2292ff"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable classification model that is likely to outperform logistic regression, such as a Naive Bayes classifier, Support Vector Machine (SVM), or a deep learning model like a recurrent neural network (RNN) or transformer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "484afecb",
        "outputId": "cd09d585-19aa-4f2e-fc36-351351c70da8"
      },
      "source": [
        "print(\"Two classification model types that are likely to outperform logistic regression on this text classification task are:\")\n",
        "print(\"\\n1. Multinomial Naive Bayes:\")\n",
        "print(\"   Justification: Naive Bayes classifiers, particularly Multinomial Naive Bayes, are well-suited for text classification tasks due to their simplicity and efficiency. They work well with high-dimensional sparse data like TF-IDF matrices. While based on a naive assumption of independence, they often perform surprisingly well in practice and can serve as a strong baseline or even outperform more complex models in certain scenarios, making them a good candidate to potentially outperform logistic regression.\")\n",
        "print(\"\\n2. Linear Support Vector Machine (SVM):\")\n",
        "print(\"   Justification: Linear SVMs are powerful linear classifiers that are effective in high-dimensional spaces. They aim to find the hyperplane that maximally separates the different classes. SVMs are known for their good performance on text classification tasks and can handle large feature sets like those generated by TF-IDF. The ability of SVMs to find an optimal separating hyperplane can lead to better generalization compared to logistic regression, making them a strong contender to achieve higher accuracy.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two classification model types that are likely to outperform logistic regression on this text classification task are:\n",
            "\n",
            "1. Multinomial Naive Bayes:\n",
            "   Justification: Naive Bayes classifiers, particularly Multinomial Naive Bayes, are well-suited for text classification tasks due to their simplicity and efficiency. They work well with high-dimensional sparse data like TF-IDF matrices. While based on a naive assumption of independence, they often perform surprisingly well in practice and can serve as a strong baseline or even outperform more complex models in certain scenarios, making them a good candidate to potentially outperform logistic regression.\n",
            "\n",
            "2. Linear Support Vector Machine (SVM):\n",
            "   Justification: Linear SVMs are powerful linear classifiers that are effective in high-dimensional spaces. They aim to find the hyperplane that maximally separates the different classes. SVMs are known for their good performance on text classification tasks and can handle large feature sets like those generated by TF-IDF. The ability of SVMs to find an optimal separating hyperplane can lead to better generalization compared to logistic regression, making them a strong contender to achieve higher accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bf682a1"
      },
      "source": [
        "## Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6dqW-UDtHE1",
        "outputId": "f59d8a9c-441b-408d-f5ad-55c14db41451"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "for label in y_train.columns:\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train[label])\n",
        "    print(f\"Model trained for label: {label}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained for label: toxic\n",
            "Model trained for label: severe_toxic\n",
            "Model trained for label: obscene\n",
            "Model trained for label: threat\n",
            "Model trained for label: insult\n",
            "Model trained for label: identity_hate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca322a36"
      },
      "source": [
        "## Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d263e6e",
        "outputId": "fbe25e05-3110-45e0-b0b3-2259324d2ed6"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "results = {}\n",
        "\n",
        "for label in y_test.columns:\n",
        "    print(f\"Evaluating model for label: {label}\")\n",
        "    # The model variable from the previous step holds the last trained model.\n",
        "    # Need to retrain or load models if not already stored in a list/dict\n",
        "    # For simplicity in this example, we'll retrain the model for each label.\n",
        "    # In a real scenario, you would store trained models.\n",
        "    model = MultinomialNB()\n",
        "    model.fit(X_train, y_train[label])\n",
        "\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test[label], y_pred)\n",
        "    precision = precision_score(y_test[label], y_pred, zero_division=0)\n",
        "    recall = recall_score(y_test[label], y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test[label], y_pred, zero_division=0)\n",
        "\n",
        "    # For AUC, we need probability scores\n",
        "    try:\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(y_test[label], y_pred_proba)\n",
        "    except AttributeError:\n",
        "        auc = \"N/A (Model does not support predict_proba)\"\n",
        "        print(f\"  AUC not calculated for {label} as model lacks predict_proba.\")\n",
        "\n",
        "\n",
        "    results[label] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-score: {f1:.4f}\")\n",
        "    if auc != \"N/A (Model does not support predict_proba)\":\n",
        "        print(f\"  AUC: {auc:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Optional: Calculate and print average metrics (e.g., macro F1-score)\n",
        "# Note: Macro averaging is sensitive to class imbalance. Weighted average might be more appropriate.\n",
        "# Calculate macro F1 for comparison across labels.\n",
        "macro_f1_sum = 0\n",
        "auc_sum = 0\n",
        "auc_count = 0\n",
        "\n",
        "for label, metrics in results.items():\n",
        "    macro_f1_sum += metrics['f1_score']\n",
        "    if isinstance(metrics['auc'], float):\n",
        "        auc_sum += metrics['auc']\n",
        "        auc_count += 1\n",
        "\n",
        "average_macro_f1 = macro_f1_sum / len(results)\n",
        "average_auc = auc_sum / auc_count if auc_count > 0 else \"N/A\"\n",
        "\n",
        "print(f\"\\nAverage Macro F1-score across all labels: {average_macro_f1:.4f}\")\n",
        "if average_auc != \"N/A\":\n",
        "     print(f\"Average AUC across labels (where calculable): {average_auc:.4f}\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model for label: toxic\n",
            "  Accuracy: 0.9217\n",
            "  Precision: 0.9115\n",
            "  Recall: 0.2022\n",
            "  F1-score: 0.3310\n",
            "  AUC: 0.8895\n",
            "------------------------------\n",
            "Evaluating model for label: severe_toxic\n",
            "  Accuracy: 0.9896\n",
            "  Precision: 0.2105\n",
            "  Recall: 0.0125\n",
            "  F1-score: 0.0235\n",
            "  AUC: 0.8853\n",
            "------------------------------\n",
            "Evaluating model for label: obscene\n",
            "  Accuracy: 0.9532\n",
            "  Precision: 0.8553\n",
            "  Recall: 0.1551\n",
            "  F1-score: 0.2626\n",
            "  AUC: 0.8887\n",
            "------------------------------\n",
            "Evaluating model for label: threat\n",
            "  Accuracy: 0.9974\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-score: 0.0000\n",
            "  AUC: 0.7973\n",
            "------------------------------\n",
            "Evaluating model for label: insult\n",
            "  Accuracy: 0.9522\n",
            "  Precision: 0.7418\n",
            "  Recall: 0.0836\n",
            "  F1-score: 0.1503\n",
            "  AUC: 0.8777\n",
            "------------------------------\n",
            "Evaluating model for label: identity_hate\n",
            "  Accuracy: 0.9903\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-score: 0.0000\n",
            "  AUC: 0.7888\n",
            "------------------------------\n",
            "\n",
            "Average Macro F1-score across all labels: 0.1279\n",
            "Average AUC across labels (where calculable): 0.8546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8af31c7f"
      },
      "source": [
        "## Hyperparameter tuning (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "240f6ef7",
        "outputId": "338ba7eb-6855-499a-f94c-469d05fd9772"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Define the parameter grid to search\n",
        "param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 5.0, 10.0]}\n",
        "\n",
        "# Choose a scoring metric (macro F1 is good for imbalanced data)\n",
        "# We will tune for each label individually due to the multi-label nature\n",
        "scoring_metric = 'f1_macro'\n",
        "\n",
        "tuned_models = {}\n",
        "tuning_results = {}\n",
        "\n",
        "# Tune for each label\n",
        "for label in y_train.columns:\n",
        "    print(f\"Tuning hyperparameters for label: {label}\")\n",
        "\n",
        "    # Instantiate the Multinomial Naive Bayes model\n",
        "    nb_model = MultinomialNB()\n",
        "\n",
        "    # Instantiate GridSearchCV\n",
        "    grid_search = GridSearchCV(estimator=nb_model,\n",
        "                               param_grid=param_grid,\n",
        "                               scoring=scoring_metric,\n",
        "                               cv=5,  # 5-fold cross-validation\n",
        "                               verbose=1,\n",
        "                               n_jobs=-1) # Use all available cores\n",
        "\n",
        "    # Fit GridSearchCV to the training data for the current label\n",
        "    grid_search.fit(X_train, y_train[label])\n",
        "\n",
        "    # Store the best model and best parameters\n",
        "    tuned_models[label] = grid_search.best_estimator_\n",
        "    tuning_results[label] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'best_score': grid_search.best_score_\n",
        "    }\n",
        "\n",
        "    print(f\"Finished tuning for label: {label}\")\n",
        "    print(f\"Best parameters: {tuning_results[label]['best_params']}\")\n",
        "    print(f\"Best cross-validation score ({scoring_metric}): {tuning_results[label]['best_score']:.4f}\")\n",
        "    print(\"-\" * 30)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning hyperparameters for label: toxic\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Finished tuning for label: toxic\n",
            "Best parameters: {'alpha': 0.01}\n",
            "Best cross-validation score (f1_macro): 0.8190\n",
            "------------------------------\n",
            "Tuning hyperparameters for label: severe_toxic\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Finished tuning for label: severe_toxic\n",
            "Best parameters: {'alpha': 0.01}\n",
            "Best cross-validation score (f1_macro): 0.6850\n",
            "------------------------------\n",
            "Tuning hyperparameters for label: obscene\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Finished tuning for label: obscene\n",
            "Best parameters: {'alpha': 0.01}\n",
            "Best cross-validation score (f1_macro): 0.8293\n",
            "------------------------------\n",
            "Tuning hyperparameters for label: threat\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Finished tuning for label: threat\n",
            "Best parameters: {'alpha': 0.01}\n",
            "Best cross-validation score (f1_macro): 0.5746\n",
            "------------------------------\n",
            "Tuning hyperparameters for label: insult\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Finished tuning for label: insult\n",
            "Best parameters: {'alpha': 0.01}\n",
            "Best cross-validation score (f1_macro): 0.7884\n",
            "------------------------------\n",
            "Tuning hyperparameters for label: identity_hate\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Finished tuning for label: identity_hate\n",
            "Best parameters: {'alpha': 0.01}\n",
            "Best cross-validation score (f1_macro): 0.6173\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f75f92b",
        "outputId": "ee1b00cb-0472-4a91-82a5-259076bcdea4"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "tuned_results_test = {}\n",
        "\n",
        "# Evaluate the tuned models on the test set\n",
        "for label, model in tuned_models.items():\n",
        "    print(f\"Evaluating tuned model for label: {label}\")\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test[label], y_pred)\n",
        "    precision = precision_score(y_test[label], y_pred, zero_division=0)\n",
        "    recall = recall_score(y_test[label], y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test[label], y_pred, zero_division=0)\n",
        "\n",
        "    # For AUC, we need probability scores\n",
        "    try:\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        auc = roc_auc_score(y_test[label], y_pred_proba)\n",
        "    except AttributeError:\n",
        "        auc = \"N/A (Model does not support predict_proba)\"\n",
        "        print(f\"  AUC not calculated for {label} as model lacks predict_proba.\")\n",
        "\n",
        "    tuned_results_test[label] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-score: {f1:.4f}\")\n",
        "    if auc != \"N/A (Model does not support predict_proba)\":\n",
        "        print(f\"  AUC: {auc:.4f}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "# Optional: Calculate and print average metrics (e.g., macro F1-score) for tuned models\n",
        "macro_f1_sum_tuned = 0\n",
        "auc_sum_tuned = 0\n",
        "auc_count_tuned = 0\n",
        "\n",
        "for label, metrics in tuned_results_test.items():\n",
        "    macro_f1_sum_tuned += metrics['f1_score']\n",
        "    if isinstance(metrics['auc'], float):\n",
        "        auc_sum_tuned += metrics['auc']\n",
        "        auc_count_tuned += 1\n",
        "\n",
        "average_macro_f1_tuned = macro_f1_sum_tuned / len(tuned_results_test)\n",
        "average_auc_tuned = auc_sum_tuned / auc_count_tuned if auc_count_tuned > 0 else \"N/A\"\n",
        "\n",
        "print(f\"\\nAverage Macro F1-score across all labels (Tuned Models): {average_macro_f1_tuned:.4f}\")\n",
        "if average_auc_tuned != \"N/A\":\n",
        "     print(f\"Average AUC across labels (where calculable, Tuned Models): {average_auc_tuned:.4f}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating tuned model for label: toxic\n",
            "  Accuracy: 0.9444\n",
            "  Precision: 0.7758\n",
            "  Recall: 0.5900\n",
            "  F1-score: 0.6703\n",
            "  AUC: 0.9341\n",
            "------------------------------\n",
            "Evaluating tuned model for label: severe_toxic\n",
            "  Accuracy: 0.9853\n",
            "  Precision: 0.3303\n",
            "  Recall: 0.4517\n",
            "  F1-score: 0.3816\n",
            "  AUC: 0.9634\n",
            "------------------------------\n",
            "Evaluating tuned model for label: obscene\n",
            "  Accuracy: 0.9679\n",
            "  Precision: 0.7403\n",
            "  Recall: 0.6216\n",
            "  F1-score: 0.6758\n",
            "  AUC: 0.9478\n",
            "------------------------------\n",
            "Evaluating tuned model for label: threat\n",
            "  Accuracy: 0.9938\n",
            "  Precision: 0.0699\n",
            "  Recall: 0.1351\n",
            "  F1-score: 0.0922\n",
            "  AUC: 0.9510\n",
            "------------------------------\n",
            "Evaluating tuned model for label: insult\n",
            "  Accuracy: 0.9638\n",
            "  Precision: 0.6648\n",
            "  Recall: 0.5737\n",
            "  F1-score: 0.6159\n",
            "  AUC: 0.9430\n",
            "------------------------------\n",
            "Evaluating tuned model for label: identity_hate\n",
            "  Accuracy: 0.9859\n",
            "  Precision: 0.2459\n",
            "  Recall: 0.2551\n",
            "  F1-score: 0.2504\n",
            "  AUC: 0.9216\n",
            "------------------------------\n",
            "\n",
            "Average Macro F1-score across all labels (Tuned Models): 0.4477\n",
            "Average AUC across labels (where calculable, Tuned Models): 0.9435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b19313d"
      },
      "source": [
        "## Classify comments\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4c1c21f",
        "outputId": "f572baa2-04a2-42e4-9db6-fa99d92a43f2"
      },
      "source": [
        "# 1. Define a list of new comments\n",
        "new_comments = [\n",
        "    \"This is a great comment!\",\n",
        "    \"You are an idiot, this is terrible.\",\n",
        "    \"I hate everything about this.\",\n",
        "    \"What a wonderful day it is.\",\n",
        "    \"You should kill yourself.\"\n",
        "]\n",
        "\n",
        "# 2. Use the fitted tfidf_vectorizer to transform the new comments\n",
        "new_comments_tfidf = tfidf_vectorizer.transform(new_comments)\n",
        "\n",
        "# 3. Iterate through the tuned_models dictionary and predict\n",
        "classification_results = {}\n",
        "\n",
        "for i, comment in enumerate(new_comments):\n",
        "    classification_results[comment] = {}\n",
        "    for label, model in tuned_models.items():\n",
        "        # Predict the toxicity label for the transformed new comments\n",
        "        prediction = model.predict(new_comments_tfidf[i])\n",
        "        classification_results[comment][label] = prediction[0] # prediction is an array\n",
        "\n",
        "# 4. Store or print the classification results\n",
        "print(\"Classification Results for New Comments:\")\n",
        "for comment, predictions in classification_results.items():\n",
        "    print(f\"\\nComment: '{comment}'\")\n",
        "    for label, prediction in predictions.items():\n",
        "        print(f\"  {label}: {'Positive' if prediction == 1 else 'Negative'}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Results for New Comments:\n",
            "\n",
            "Comment: 'This is a great comment!'\n",
            "  toxic: Negative\n",
            "  severe_toxic: Negative\n",
            "  obscene: Negative\n",
            "  threat: Negative\n",
            "  insult: Negative\n",
            "  identity_hate: Negative\n",
            "\n",
            "Comment: 'You are an idiot, this is terrible.'\n",
            "  toxic: Positive\n",
            "  severe_toxic: Negative\n",
            "  obscene: Positive\n",
            "  threat: Negative\n",
            "  insult: Negative\n",
            "  identity_hate: Negative\n",
            "\n",
            "Comment: 'I hate everything about this.'\n",
            "  toxic: Negative\n",
            "  severe_toxic: Negative\n",
            "  obscene: Negative\n",
            "  threat: Negative\n",
            "  insult: Negative\n",
            "  identity_hate: Negative\n",
            "\n",
            "Comment: 'What a wonderful day it is.'\n",
            "  toxic: Negative\n",
            "  severe_toxic: Negative\n",
            "  obscene: Negative\n",
            "  threat: Negative\n",
            "  insult: Negative\n",
            "  identity_hate: Negative\n",
            "\n",
            "Comment: 'You should kill yourself.'\n",
            "  toxic: Positive\n",
            "  severe_toxic: Negative\n",
            "  obscene: Positive\n",
            "  threat: Positive\n",
            "  insult: Negative\n",
            "  identity_hate: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d786b5eb"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset contains 159,571 comments with associated toxicity labels (`toxic`, `severe_toxic`, `obscene`, `threat`, `insult`, `identity_hate`), a `clean` label, and `char_length`.\n",
        "*   Text data was preprocessed and converted into numerical representations using TF-IDF vectorization with specific parameters (min\\_df=3, ngram\\_range=(1, 3)).\n",
        "*   The data was split into training (80%) and testing (20%) sets.\n",
        "*   Multinomial Naive Bayes was chosen as a suitable model for this multi-label text classification task, trained independently for each of the six toxicity labels.\n",
        "*   Initial evaluation showed varying performance across labels, with better results for more frequent classes (`toxic`, `obscene`) and low F1-scores for rare classes (`severe_toxic`, `threat`, `identity_hate`) despite reasonable AUCs.\n",
        "*   Hyperparameter tuning for the Multinomial Naive Bayes models using `GridSearchCV` found an optimal `alpha` of 0.01 across all labels within the tested range.\n",
        "*   Evaluation of the tuned models on the test set showed an average Macro F1-score of approximately 0.4477 and an average AUC of approximately 0.9435.\n",
        "*   The tuned models were successfully used to classify new comments, providing predictions for each toxicity label.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The significant class imbalance strongly impacts the F1-score, particularly for rare toxicity types. Further steps could explore techniques to address this imbalance, such as oversampling minority classes or using different evaluation metrics like Weighted F1-score or metrics specifically designed for imbalanced data.\n",
        "*   While AUC is high, indicating good discriminatory power, the low F1 for rare classes suggests the model has difficulty setting a threshold for positive predictions. Investigating different prediction probability thresholds or using models better suited for highly imbalanced multi-label classification could be beneficial.\n"
      ]
    }
  ]
}